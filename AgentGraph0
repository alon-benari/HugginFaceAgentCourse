
%pip install -q langgraph langchain_openai langchain_huggingface
%pip install transformers

import os

from typing import TypedDict, List, Dict, Any, Optional
from langgraph.graph import StateGraph, START, END
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

## simple example

class EmailState(TypedDict):
    # The email being processed
    email: Dict[str, Any]  # Contains subject, sender, body, etc.

    # Category of the email (inquiry, complaint, etc.)
    email_category: Optional[str]

    # Reason why the email was marked as spam
    spam_reason: Optional[str]

    # Analysis and decisions
    is_spam: Optional[bool]
    
    # Response generation
    email_draft: Optional[str]
    
    # Processing metadata
    messages: List[Dict[str, Any]]  # Track conversation with LLM for analysis





    def read_email(state: EmailState):
        """Alfred reads and logs the incoming email"""
        email = state["email"]
        
        # Here we might do some initial preprocessing
        print(f"Alfred is processing an email from {email['sender']} with subject: {email['subject']}")
        
        # No state changes needed here
        return {}


    

def read_email(state: EmailState):
    """Alfred reads and logs the incoming email"""
    email = state["email"]
    
    # Here we might do some initial preprocessing
    print(f"Alfred is processing an email from {email['sender']} with subject: {email['subject']}")
    
    # No state changes needed here
    return {}

def classify_email(state: EmailState):
    """Alfred uses an LLM to determine if the email is spam or legitimate"""
    email = state["email"]
    
    # Prepare our prompt for the LLM
    prompt = f"""
    As Alfred the butler, analyze this email and determine if it is spam or legitimate.
    
    Email:
    From: {email['sender']}
    Subject: {email['subject']}
    Body: {email['body']}
    
    First, determine if this email is spam. If it is spam, explain why.
    If it is legitimate, categorize it (inquiry, complaint, thank you, etc.).
    """
    
    # Call the LLM
    messages = [HumanMessage(content=prompt)]
    response = model.invoke(messages)
    
    # Simple logic to parse the response (in a real app, you'd want more robust parsing)
    response_text = response.content.lower()
    is_spam = "spam" in response_text and "not spam" not in response_text
    
    # Extract a reason if it's spam
    spam_reason = None
    if is_spam and "reason:" in response_text:
        spam_reason = response_text.split("reason:")[1].strip()
    
    # Determine category if legitimate
    email_category = None
    if not is_spam:
        categories = ["inquiry", "complaint", "thank you", "request", "information"]
        for category in categories:
            if category in response_text:
                email_category = category
                break
    
    # Update messages for tracking
    new_messages = state.get("messages", []) + [
        {"role": "user", "content": prompt},
        {"role": "assistant", "content": response.content}
    ]
    
    # Return state updates
    return {
        "is_spam": is_spam,
        "spam_reason": spam_reason,
        "email_category": email_category,
        "messages": new_messages
    }

def handle_spam(state: EmailState):
    """Alfred discards spam email with a note"""
    print(f"Alfred has marked the email as spam. Reason: {state['spam_reason']}")
    print("The email has been moved to the spam folder.")
    
    # We're done processing this email
    return {}

def draft_response(state: EmailState):
    """Alfred drafts a preliminary response for legitimate emails"""
    email = state["email"]
    category = state["email_category"] or "general"
    
    # Prepare our prompt for the LLM
    prompt = f"""
    As Alfred the butler, draft a polite preliminary response to this email.
    
    Email:
    From: {email['sender']}
    Subject: {email['subject']}
    Body: {email['body']}
    
    This email has been categorized as: {category}
    
    Draft a brief, professional response that Mr. Hugg can review and personalize before sending.
    """
    
    # Call the LLM
    messages = [HumanMessage(content=prompt)]
    response = model.invoke(messages)
    
    # Update messages for tracking
    new_messages = state.get("messages", []) + [
        {"role": "user", "content": prompt},
        {"role": "assistant", "content": response.content}
    ]
    
    # Return state updates
    return {
        "email_draft": response.content,
        "messages": new_messages
    }

def notify_mr_hugg(state: EmailState):
    """Alfred notifies Mr. Hugg about the email and presents the draft response"""
    email = state["email"]
    
    print("\n" + "="*50)
    print(f"Sir, you've received an email from {email['sender']}.")
    print(f"Subject: {email['subject']}")
    print(f"Category: {state['email_category']}")
    print("\nI've prepared a draft response for your review:")
    print("-"*50)
    print(state["email_draft"])
    print("="*50 + "\n")
    
    # We're done processing this email
    return {}

    def route_email(state: EmailState) -> str:
        """Determine the next step based on spam classification"""
        if state["is_spam"]:
            return "spam"
        else:
            return "legitimate"

    
# Create the graph
email_graph = StateGraph(EmailState)

# Add nodes
email_graph.add_node("read_email", read_email)
email_graph.add_node("classify_email", classify_email)
email_graph.add_node("handle_spam", handle_spam)
email_graph.add_node("draft_response", draft_response)
email_graph.add_node("notify_mr_hugg", notify_mr_hugg)

# Start the edges
email_graph.add_edge(START, "read_email")
# Add edges - defining the flow
email_graph.add_edge("read_email", "classify_email")

# Add conditional branching from classify_email
email_graph.add_conditional_edges(
    "classify_email",
    route_email,
    {
        "spam": "handle_spam",
        "legitimate": "draft_response"
    }
)

# Add the final edges
email_graph.add_edge("handle_spam", END)
email_graph.add_edge("draft_response", "notify_mr_hugg")
email_graph.add_edge("notify_mr_hugg", END)

# Compile the graph
compiled_graph = email_graph.compile()


## View Graph
# View
from IPython.display import Image, display
from langgraph.graph import StateGraph, START, END
display(Image(compiled_graph.get_graph().draw_mermaid_png()))



# Example legitimate email
legitimate_email = {
    "sender": "john.smith@example.com",
    "subject": "Question about your services",
    "body": "Dear Mr. Hugg, I was referred to you by a colleague and I'm interested in learning more about your consulting services. Could we schedule a call next week? Best regards, John Smith"
}

# Example spam email
spam_email = {
    "sender": "winner@lottery-intl.com",
    "subject": "YOU HAVE WON $5,000,000!!!",
    "body": "CONGRATULATIONS! You have been selected as the winner of our international lottery! To claim your $5,000,000 prize, please send us your bank details and a processing fee of $100."
}

# Process the legitimate email
print("\nProcessing legitimate email...")
legitimate_result = compiled_graph.invoke({
    "email": legitimate_email,
    "is_spam": None,
    "spam_reason": None,
    "email_category": None,
    "email_draft": None,
    "messages": []
})

# Process the spam email
print("\nProcessing spam email...")
spam_result = compiled_graph.invoke({
    "email": spam_email,
    "is_spam": None,
    "spam_reason": None,
    "email_category": None,
    "email_draft": None,
    "messages": []
})


##------Using HugginFace only-------------
rom transformers import pipeline
from typing import TypedDict, List, Dict, Any, Optional
from langgraph.graph import StateGraph, START, END
import langgraph
import torch

## create state schema
class EmailState(TypedDict):
    # The email being processed
    #email: Dict[str, Any]  # Contains subject, sender, body, etc.

    # Category of the email (inquiry, complaint, etc.)
    #email_category: Optional[str]

    # Reason why the email was marked as spam
    #spam_reason: Optional[str]

    # Analysis and decisions
    is_spam: Optional[bool]
    
    # Response generation
    #email_draft: Optional[str]
    
    # Processing metadata
   #messages: List[Dict[str, Any]]  # Track conversation with LLM for analysis


# Load a Hugging Face text classification pipeline
#spam_classifier = pipeline("text-classification", model="facebook/bart-large-mnli")



# Define spam classification function
def classify_spam(state:EmailState):
    text = state["text"]
    result = spam_classifier(text)
    label = result[0]["label"]
    return {"classification": "Spam" if "spam" in label.lower() else "Ham"}

# Create a LangGraph state graph
workflow = StateGraph(EmailState)

# Add spam classification step
workflow.add_node("Spam Classification", classify_spam)

# Define edges (for more complex workflows)
workflow.set_entry_point("Spam Classification")

# Build the graph
graph = workflow.compile()

# Example input texts
input_texts = [
    {"text": "Congratulations! You've won a free iPhone. Click here to claim your prize."},
    {"text": "Hi, can we schedule a meeting for tomorrow?"}
]

# Run the classifier
for text_data in input_texts:
    output = graph.invoke(text_data)
    print(f"Text: {text_data['text']}")
    print(f"Classification: {output['classification']}")
    print()



    from transformers import pipeline
import langgraph
from langchain.pydantic_v1 import BaseModel

# Load Hugging Face spam classification model (replace with a more spam-specific model if needed)
#spam_classifier = pipeline("text-classification", model="facebook/bart-large-mnli")
spam_classifier = pipeline("text-classification", model="cybert79/spamai")
 

# Define schema for state management
class SpamClassifierState(BaseModel):
    text: str
    classification: str = None
    action: str = None

# Define spam classification function
def classify_spam(state: SpamClassifierState):
    result = spam_classifier(state.text)
    label = result[0]["label"]
    state.classification = "Spam" if "spam" in label.lower() else "Ham"
    return state

# Define message routing logic based on classification result
def handle_spam(state: SpamClassifierState):
    state.action = "Flagged as Spam" if state.classification == "Spam" else "Proceed as Normal"
    return state

# Create a StateGraph with schema
workflow = langgraph.graph.StateGraph(SpamClassifierState)

# Add nodes
workflow.add_node("Spam Classification", classify_spam)
workflow.add_node("Message Handling", handle_spam)

# Define transitions
workflow.set_entry_point("Spam Classification")
workflow.add_edge("Spam Classification", "Message Handling")

# Build the graph
graph = workflow.compile()

# Example input texts
input_texts = [
    SpamClassifierState(text="Congratulations! You've won a million dollars. Click here to claim your prize."),
    SpamClassifierState(text="Hi, can we schedule a meeting for tomorrow?")
]

# Run the classifier
for text_data in input_texts:
    output = graph.invoke(text_data)
    print(f"Text: {output["text"]}")
    print(f"Classification: {output["classification"]}")
    print(f"Action Taken: {output["action"]}")
    #print(output)




#------------Another example
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# Load a pre-trained model for text classification
model_name = "bhadresh-savani/distilbert-base-uncased-emotion"  # Replace with a spam classification model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# Example input text
texts = [
    "Congratulations! You've won a free iPhone. Click here to claim your prize.",
    "Hi, can we schedule a meeting for tomorrow?"
]

# Tokenize the input texts
inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt")

# Perform classification
outputs = model(**inputs)
predictions = torch.argmax(outputs.logits, dim=1)

# Map labels to categories
label_map = {0: "ham", 1: "spam"}  # Update based on your model's labels
results = [label_map[prediction.item()] for prediction in predictions]

# Print results
for text, result in zip(texts, results):
    print(f"Text: {text}")
    print(f"Classification: {result}")
    print()